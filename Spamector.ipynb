{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Spamector Logo](jupyter_images/logo.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "***\n",
    "Text mining is a wide field which has gained popularity with the huge text data being generated. \n",
    "Automation of several applications like sentiment analysis, document classification, topic classification, text summarization, and machine translation has been done using machine learning models.\n",
    "\n",
    "Spam filtering is an example of document classification task which involves classifying an email as spam or non-spam (a.k.a. ham) mail.\n",
    "\n",
    "In this notebook, we’ll go step by step on how to implement a spam detection system, and also how to deploy our algorithm into an API then to a beautiful web interface."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "***\n",
    "1. [Obtaining Data](#Obtaining-Data)\n",
    "2. [Exploring Data](#Exploring-Data)\n",
    "3. [Preparing Data](#Preparing-Data)\n",
    "4. [Modeling Data](#Modeling-Data)\n",
    "5. [Evaluation](#Evaluation)\n",
    "6. [Deployment](#Deployment)\n",
    "7. [Web Application](#Web-Application)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's Get Started\n",
    "\n",
    "![One Bite](jupyter_images/one_bite.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtaining Data \n",
    "***\n",
    "* We will be using the Enron Spam dataset in its pre-processed form. You can get it from here : http://www.aueb.gr/users/ion/data/enron-spam\n",
    "![Enron](jupyter_images/enron.png)\n",
    "\n",
    "## Exploring Data \n",
    "***\n",
    "* All the spam emails are in a folder called spam, while non-spam are in ham.\n",
    "\n",
    "**Step 1**\n",
    "\n",
    "* Printing all the directories, sub directories, and files in our data folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let’s import the necessary libraries.\n",
    "from nltk.classify import NaiveBayesClassifier, SklearnClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "import nltk.classify.util\n",
    "from nltk.corpus import movie_reviews\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.svm import LinearSVC\n",
    "import os, random, time, pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is our data directory\n",
    "data_directory = \"data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data ['enron1', 'enron2', 'enron3', 'enron4', 'enron5', 'enron6'] 0\n",
      "data\\enron1 ['ham', 'spam'] 0\n",
      "data\\enron1\\ham [] 3672\n",
      "data\\enron1\\spam [] 1500\n",
      "data\\enron2 ['ham', 'spam'] 0\n",
      "data\\enron2\\ham [] 4361\n",
      "data\\enron2\\spam [] 1496\n",
      "data\\enron3 ['ham', 'spam'] 0\n",
      "data\\enron3\\ham [] 4012\n",
      "data\\enron3\\spam [] 1500\n",
      "data\\enron4 ['ham', 'spam'] 0\n",
      "data\\enron4\\ham [] 1500\n",
      "data\\enron4\\spam [] 4500\n",
      "data\\enron5 ['ham', 'spam'] 0\n",
      "data\\enron5\\ham [] 1500\n",
      "data\\enron5\\spam [] 3675\n",
      "data\\enron6 ['ham', 'spam'] 0\n",
      "data\\enron6\\ham [] 1500\n",
      "data\\enron6\\spam [] 4500\n"
     ]
    }
   ],
   "source": [
    "# Let’s now loop through all the directories, subdirectories, and files in the above folder, and print them. \n",
    "# For files, we print the count.\n",
    "for directories, subdirs, files in os.walk(data_directory):\n",
    "    print(directories, subdirs, len(files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2**\n",
    "\n",
    "* Printing the files when we are in the ham or spam folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data\\enron1\\ham [] 3672\n",
      "data\\enron1\\spam [] 1500\n",
      "data\\enron2\\ham [] 4361\n",
      "data\\enron2\\spam [] 1496\n",
      "data\\enron3\\ham [] 4012\n",
      "data\\enron3\\spam [] 1500\n",
      "data\\enron4\\ham [] 1500\n",
      "data\\enron4\\spam [] 4500\n",
      "data\\enron5\\ham [] 1500\n",
      "data\\enron5\\spam [] 3675\n",
      "data\\enron6\\ham [] 1500\n",
      "data\\enron6\\spam [] 4500\n"
     ]
    }
   ],
   "source": [
    "# Now instead of printing all files and folders, we only print the files when we are in the ham or spam folder.\n",
    "for directories, subdirs, files in os.walk(data_directory):\n",
    "    if (os.path.split(directories)[1]  == 'spam'):\n",
    "        print(directories, subdirs, len(files))\n",
    "\n",
    "    if (os.path.split(directories)[1]  == 'ham'):\n",
    "        print(directories, subdirs, len(files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why does it matter?**\n",
    "\n",
    "* When we start reading the files, we want to make sure we are only reading them from the spam and ham folders.\n",
    "\n",
    "**Step 3**\n",
    "\n",
    "* Reading all the files in those folders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject: christmas tree farm pictures\n",
      "\n",
      "--------- \n",
      "Subject: dobmeos with hgh my energy level has gone up ! stukm\n",
      "introducing\n",
      "doctor - formulated\n",
      "hgh\n",
      "human growth hormone - also called hgh\n",
      "is referred to in medical science as the master hormone . it is very plentiful\n",
      "when we are young , but near the age of twenty - one our bodies begin to produce\n",
      "less of it . by the time we are forty nearly everyone is deficient in hgh ,\n",
      "and at eighty our production has normally diminished at least 90 - 95 % .\n",
      "advantages of hgh :\n",
      "- increased muscle strength\n",
      "- loss in body fat\n",
      "- increased bone density\n",
      "- lower blood pressure\n",
      "- quickens wound healing\n",
      "- reduces cellulite\n",
      "- improved vision\n",
      "- wrinkle disappearance\n",
      "- increased skin thickness texture\n",
      "- increased energy levels\n",
      "- improved sleep and emotional stability\n",
      "- improved memory and mental alertness\n",
      "- increased sexual potency\n",
      "- resistance to common illness\n",
      "- strengthened heart muscle\n",
      "- controlled cholesterol\n",
      "- controlled mood swings\n",
      "- new hair growth and color restore\n",
      "read\n",
      "more at this website\n",
      "unsubscribe\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We read the files and append them to the ham and spam list\n",
    "hamList = []\n",
    "spamList = []\n",
    "for directories, subdirs, files in os.walk(data_directory):\n",
    "    if (os.path.split(directories)[1]  == 'ham'):\n",
    "        for fileName in files:      \n",
    "            with open(os.path.join(directories, fileName), encoding=\"latin-1\") as f:\n",
    "                message = f.read()\n",
    "                hamList.append(message)\n",
    "\n",
    "    if (os.path.split(directories)[1]  == 'spam'):\n",
    "        for fileName in files:\n",
    "            with open(os.path.join(directories, fileName), encoding=\"latin-1\") as f:\n",
    "                message = f.read()\n",
    "                spamList.append(message)\n",
    "print(hamList[0])\n",
    "print('--------- ')\n",
    "print(spamList[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Why **encoding=\"latin-1\"**?\n",
    "\n",
    "Some files contain special characters, and if Python 2 would allow us to get away with this, Python 3 won’t. So the way to stop Python from throwing a Unicode error is to add an encoding.\n",
    "\n",
    "Reference - [Processing Text Files in Python 3](http://python-notes.curiousefficiency.org/en/latest/python3/text_file_processing.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Data \n",
    "***\n",
    "* Now we need to prepare our data for the classifiers. But which classifiers we will use?\n",
    "\n",
    "The naïve Bayes and support vector machine are the typical generative and discriminative classification models respectively, which are two popular classification approaches. We will use both of them and then compare which model is better.\n",
    "\n",
    "**- Naive Bayes?**\n",
    "\n",
    "Naive Bayes classifiers are a popular statistical technique of e-mail filtering. They typically use bag of words features to identify spam e-mail, an approach commonly used in text classification.\n",
    "\n",
    "Naive Bayes classifiers work by correlating the use of tokens (typically words, or sometimes other things), with spam and non-spam e-mails and then using Bayes' theorem to calculate a probability that an email is or is not spam.\n",
    "\n",
    "**- How does it work?**\n",
    "![Naive Bayes Diagram](jupyter_images/naive_bayes.png)\n",
    "\n",
    "**- Formula?**\n",
    "![Naive Bayes Formula](jupyter_images/formula.png)\n",
    "\n",
    "**- Support Vector Machine?**\n",
    "\n",
    "Support Vector Machine (SVM) is a supervised machine learning algorithm which can be used for both classification or regression challenges. However,  it is mostly used in classification problems. In our case, we will use Linear Support Vector Classifier from Sklearn.\n",
    "![SVM](jupyter_images/sphx_glr_plot_iris_0012.png)\n",
    "\n",
    "\n",
    "\n",
    "- Reference\n",
    "\n",
    "[Naive Bayes spam filtering](https://en.wikipedia.org/wiki/Naive_Bayes_spam_filtering)\n",
    "\n",
    "[Naïve Bayes vs. Support Vector Machine: Resilience to Missing Data](https://link.springer.com/chapter/10.1007/978-3-642-23887-1_86)\n",
    "\n",
    "[scikit-learn.org - Support Vector Machines](http://scikit-learn.org/stable/modules/svm.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**Step 1**\n",
    "\n",
    "* Naive Bayes expects the input in a particular format: \n",
    "\n",
    "{Word1: True, Word2: True, Word3: True}\n",
    "\n",
    "* So here, we are just creating a dictionary that returns True for each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createWordFeatures(words):\n",
    "    my_dict = dict( [ (word, True) for word in words] )\n",
    "    return my_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Let's test our function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MIS 5470': True,\n",
       " 'R': True,\n",
       " 'and': True,\n",
       " 'class': True,\n",
       " 'has': True,\n",
       " 'modules': True,\n",
       " 'python': True,\n",
       " 'the': True}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "createWordFeatures([\"the\", \"MIS 5470\", \"class\", \"has\", \"python\", \"and\",\"R\",\"modules\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2**\n",
    "\n",
    "* Now with the help of *createWordFeatures()* function, we will append a “ham” at the end. This is to tell the algorithm that this text is of type ham, and we’ll do the same for “spam”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'Subject': True, ':': True, 'christmas': True, 'tree': True, 'farm': True, 'pictures': True}, 'ham')\n",
      "({'Subject': True, ':': True, 'dobmeos': True, 'with': True, 'hgh': True, 'my': True, 'energy': True, 'level': True, 'has': True, 'gone': True, 'up': True, '!': True, 'stukm': True, 'introducing': True, 'doctor': True, '-': True, 'formulated': True, 'human': True, 'growth': True, 'hormone': True, 'also': True, 'called': True, 'is': True, 'referred': True, 'to': True, 'in': True, 'medical': True, 'science': True, 'as': True, 'the': True, 'master': True, '.': True, 'it': True, 'very': True, 'plentiful': True, 'when': True, 'we': True, 'are': True, 'young': True, ',': True, 'but': True, 'near': True, 'age': True, 'of': True, 'twenty': True, 'one': True, 'our': True, 'bodies': True, 'begin': True, 'produce': True, 'less': True, 'by': True, 'time': True, 'forty': True, 'nearly': True, 'everyone': True, 'deficient': True, 'and': True, 'at': True, 'eighty': True, 'production': True, 'normally': True, 'diminished': True, 'least': True, '90': True, '95': True, '%': True, 'advantages': True, 'increased': True, 'muscle': True, 'strength': True, 'loss': True, 'body': True, 'fat': True, 'bone': True, 'density': True, 'lower': True, 'blood': True, 'pressure': True, 'quickens': True, 'wound': True, 'healing': True, 'reduces': True, 'cellulite': True, 'improved': True, 'vision': True, 'wrinkle': True, 'disappearance': True, 'skin': True, 'thickness': True, 'texture': True, 'levels': True, 'sleep': True, 'emotional': True, 'stability': True, 'memory': True, 'mental': True, 'alertness': True, 'sexual': True, 'potency': True, 'resistance': True, 'common': True, 'illness': True, 'strengthened': True, 'heart': True, 'controlled': True, 'cholesterol': True, 'mood': True, 'swings': True, 'new': True, 'hair': True, 'color': True, 'restore': True, 'read': True, 'more': True, 'this': True, 'website': True, 'unsubscribe': True}, 'spam')\n"
     ]
    }
   ],
   "source": [
    "# Fist, using word_tokenize(), we break the sentences into words.\n",
    "# Second, we use createWordFeatures() function.\n",
    "hamList = []\n",
    "spamList = []\n",
    "for directories, subdirs, files in os.walk(data_directory):\n",
    "    if (os.path.split(directories)[1]  == 'ham'):\n",
    "        for fileName in files:      \n",
    "            with open(os.path.join(directories, fileName), encoding=\"latin-1\") as f:\n",
    "                message = f.read()               \n",
    "                wordList = word_tokenize(message)               \n",
    "                hamList.append((createWordFeatures(wordList), \"ham\"))\n",
    "    \n",
    "    if (os.path.split(directories)[1]  == 'spam'):\n",
    "        for fileName in files:\n",
    "            with open(os.path.join(directories, fileName), encoding=\"latin-1\") as f:\n",
    "                message = f.read()               \n",
    "                wordList = word_tokenize(message)               \n",
    "                spamList.append((createWordFeatures(wordList), \"spam\"))\n",
    "print(hamList[0])\n",
    "print(spamList[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3**\n",
    "\n",
    "* Merging spam and ham lists.\n",
    "* Shuffling the result to make it randomised."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of files: 33716\n"
     ]
    }
   ],
   "source": [
    "mergedList = hamList + spamList\n",
    "random.shuffle(mergedList)\n",
    "print(\"Number of files: {}\".format(len(mergedList)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling Data \n",
    "***\n",
    "**Step 1**\n",
    "\n",
    "* Creating test and train splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of files:    33716\n",
      "Number of Training files: 20229\n",
      "Number of Test files:     13487\n"
     ]
    }
   ],
   "source": [
    "# We choose to have 60% of the data as training and 40% as test.\n",
    "trainingPart = int(len(mergedList) * .6)\n",
    "trainingData = mergedList[:trainingPart]\n",
    "testData =  mergedList[trainingPart:]\n",
    "\n",
    "print(\"Total Number of files:    {}\".format(len(mergedList)))\n",
    "print(\"Number of Training files: {}\".format(len(trainingData)))\n",
    "print(\"Number of Test files:     {}\".format(len(testData)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2**\n",
    "\n",
    "* Calling the Naive Bayes classifier.\n",
    "* Finding its accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained the Naive Bayes classifier in 7.60 seconds\n",
      "Accuracy is:  98.61347964706755\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "nb_classifier = NaiveBayesClassifier.train(trainingData)\n",
    "print(\"Trained the Naive Bayes classifier in {:,.2f} seconds\".format(time.time()-start_time))\n",
    "accuracy = nltk.classify.util.accuracy(nb_classifier, testData)\n",
    "print(\"Accuracy is: \", accuracy * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We are getting an accuracy of 98%, which is Great.\n",
    "\n",
    "**Step 3**\n",
    "\n",
    "* Let's have a look at the most interesting features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "                   enron = True              ham : spam   =   3115.4 : 1.0\n",
      "                   daren = True              ham : spam   =    447.4 : 1.0\n",
      "                     php = True             spam : ham    =    363.0 : 1.0\n",
      "                     hpl = True              ham : spam   =    305.3 : 1.0\n",
      "                     ect = True              ham : spam   =    219.8 : 1.0\n",
      "                crenshaw = True              ham : spam   =    217.0 : 1.0\n",
      "                   corel = True             spam : ham    =    200.7 : 1.0\n",
      "                     713 = True              ham : spam   =    188.3 : 1.0\n",
      "              scheduling = True              ham : spam   =    181.1 : 1.0\n",
      "                     eol = True              ham : spam   =    179.0 : 1.0\n",
      "                  louise = True              ham : spam   =    177.8 : 1.0\n",
      "                 parsing = True              ham : spam   =    172.0 : 1.0\n",
      "                     sex = True             spam : ham    =    158.7 : 1.0\n",
      "               schedules = True              ham : spam   =    158.3 : 1.0\n",
      "                       \u0001 = True              ham : spam   =    154.2 : 1.0\n",
      "                      xp = True             spam : ham    =    146.7 : 1.0\n",
      "             origination = True              ham : spam   =    130.1 : 1.0\n",
      "              medication = True             spam : ham    =    122.7 : 1.0\n",
      "            prescription = True             spam : ham    =    104.7 : 1.0\n",
      "                    omit = True             spam : ham    =    104.0 : 1.0\n"
     ]
    }
   ],
   "source": [
    "nb_classifier.show_most_informative_features(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 4**\n",
    "\n",
    "* Calling the Linear SVC.\n",
    "* Finding its accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained the SVM classifier in 100.69 seconds\n",
      "Accuracy is:  98.32431230073404\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "SVM_classifier = SklearnClassifier(LinearSVC(), sparse=False).train(trainingData)\n",
    "print(\"Trained the SVM classifier in {:,.2f} seconds\".format(time.time()-start_time))\n",
    "SVM_accuracy = nltk.classify.util.accuracy(SVM_classifier, testData)\n",
    "print(\"Accuracy is: \", SVM_accuracy * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "***\n",
    "<img src=\"jupyter_images/evaluation.gif\" style=\"width: 300px;\"/>\n",
    "### Comparing Models\n",
    "![SVM_NB](jupyter_images/svm_nb.png)\n",
    "\n",
    "- We can see that the two models they have the same accuracy however the SVM takes longer time than the naïve Bayes. So, our winner is the naïve Bayes model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting New Emails\n",
    "\n",
    "* Let's clasify the below messages as spam or ham. How to do it?\n",
    "\n",
    "\n",
    "1. Break the message into words using *word_tokenzise*\n",
    "2. Generate features using *createWordFeatures*\n",
    "3. Use the *classify* function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of Nigerian Scam email\n",
    "msg1 = '''Sir, we are honourably seeking your assistance in the following ways.\n",
    "        1) To provide a Bank account where this money would be transferred to.\n",
    "        2) To serve as the guardian of this since I am a girl of 26 years.\n",
    "        Moreover Sir, we are willing to offer you 15% of the sum as compensation for effort input after the successful transfer of this fund to your designate account overseas. please feel free to contact ,me via this email address\n",
    "        wumi1000abdul@yahoo.comAnticipating to hear from you soon.Thanks and God Bless.'''\n",
    "\n",
    "# Note from Professor regarding Python + Tableau for plotting flight pattern\n",
    "msg2 = '''I've mentioned that Python is widely used for data pre-processing tasks. \n",
    "        Here's a really nice use of Python for preparing flight data for plotting in Tableau. \n",
    "        The same principles can be used for plotting all kinds of geographic related \"routes\". \n",
    "        Plus, you learn about KML files and working with spatial data.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message 1 is spam\n"
     ]
    }
   ],
   "source": [
    "words = word_tokenize(msg1)\n",
    "features = createWordFeatures(words)\n",
    "print(\"Message 1 is\" ,nb_classifier.classify(features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"jupyter_images/spam.gif\" style=\"width: 300px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message 2 is ham\n"
     ]
    }
   ],
   "source": [
    "words = word_tokenize(msg2)\n",
    "features = createWordFeatures(words)\n",
    "print(\"Message 2 is\" ,nb_classifier.classify(features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"jupyter_images/ham.gif\" style=\"width: 300px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deployment\n",
    "***\n",
    "## Saving Model\n",
    "<img src=\"jupyter_images/picklejar.png\"  style=\"width: 250px;\"/>\n",
    "* **Pickle** is the standard way of serializing objects in Python. We can use the pickle operation to serialize our machine learning algorithms and save the serialized format to a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the model to disk\n",
    "filename = 'nb_model.sav'\n",
    "pickle.dump(nb_classifier, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Model\n",
    "\n",
    "* Here we can load this file to deserialize our model and use it to make new predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded the model with Pickle in 2.54 seconds\n"
     ]
    }
   ],
   "source": [
    "# Load the model from disk\n",
    "start_time_pickle = time.time()\n",
    "loaded_model = pickle.load(open(filename, 'rb'))\n",
    "print(\"Loaded the model with Pickle in {:,.2f} seconds\".format(time.time()-start_time_pickle))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We can use as well **Sklearn’s Joblib**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['nb_model.joblib']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Saving the model with Joblib\n",
    "joblib.dump(nb_classifier,\"nb_model.joblib\", compress=1) # compression into 1 file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded the model with Joblib in 13.73 seconds\n"
     ]
    }
   ],
   "source": [
    "# Loading the model with Joblib\n",
    "start_time_joblib = time.time()\n",
    "joblib_model = joblib.load(\"nb_model.joblib\")\n",
    "print(\"Loaded the model with Joblib in {:,.2f} seconds\".format(time.time()-start_time_joblib))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pickle Vs Joblib Performance\n",
    "<img src=\"jupyter_images/diff.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Application\n",
    "***\n",
    "<img src=\"jupyter_images/web.gif\"  style=\"width: 250px;\"/>\n",
    "*  Let's now create a simple web application that would allow us to enter any message as an input and tell us if it's a spam or ham. We'll be using Flask, a Python web application micro-framework.\n",
    "\n",
    "\n",
    "**“Hello World” application in Flask**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route(\"/\")\n",
    "def hello():\n",
    "    return \"Hello 5470 World!\"\n",
    "\n",
    "#if __name__ == \"__main__\":\n",
    "#    app.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structure\n",
    "\n",
    "This is the basic structure of our web application:\n",
    "![Structure](jupyter_images/structure.png)\n",
    "\n",
    "* The **templates** folder is the place where the templates will be put. The **static** folder is the place where any files (images, css, javascript) needed by the web application will be put.\n",
    "\n",
    "* **app.py**: It's our main Python Application.\n",
    "\n",
    "* **engine.py**: It's the Python class containing the predictive model and its related functions.\n",
    "***\n",
    "## Engine ~ Spam Detector as a Class\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpamDetector:\n",
    "\n",
    "\tdef createWordFeatures(self,words):\n",
    "\t\t\"\"\"\n",
    "\t\tFunction\n",
    "\t\t--------\n",
    "\t\tcreateWordFeatures\n",
    "\n",
    "\t\tCreate a dictionary that returns True for each word.\n",
    "\n",
    "\t\tParameters\n",
    "\t\t----------\n",
    "\t\twords : list\n",
    "\t\tThe list of words\n",
    "\n",
    "\t\tReturns\n",
    "\t\t-------\n",
    "\t\tresDict : Dict\n",
    "\n",
    "\t\tExample\n",
    "\t\t-------\n",
    "\t\tcreateWordFeatures([\"the\", \"MIS 5470\", \"class\", \"has\", \"python\", \"and\",\"R\",\"modules\"])\n",
    "\t\t\n",
    "\t\t{'MIS 5470': True,\n",
    "\t\t 'R': True,\n",
    "\t\t 'and': True,\n",
    "\t\t 'class': True,\n",
    "\t\t 'has': True,\n",
    "\t\t 'modules': True,\n",
    "\t\t 'python': True,\n",
    "\t\t 'the': True}\n",
    "\t\t\"\"\"\n",
    "\t\tresDict = dict( [ (word, True) for word in words] )\n",
    "\t\treturn resDict\n",
    "\t\t\n",
    "\tdef tokenizeCreateWordFeatures(self):\n",
    "\t\t\"\"\"\n",
    "\t\tFunction\n",
    "\t\t--------\n",
    "\t\ttokenizeCreateWordFeatures\n",
    "\n",
    "\t\tAppend a \"ham\" or \"spam\" at the end of each dictionary created by createWordFeatures function. \n",
    "\t\tThis is to tell the algorithm that this text is of type ham or spam.\n",
    "\t\tMerge spam and ham lists.\n",
    "\t\tShuffle the result to make it randomised.\n",
    "\n",
    "\t\tReturns\n",
    "\t\t-------\n",
    "\t\tmergedList : list\n",
    "\n",
    "\t\tExample\n",
    "\t\t-------\n",
    "\t\toutput = tokenizeCreateWordFeatures()\n",
    "\t\tprint(output[0])\n",
    "\t\t\n",
    "\t\t({'Subject': True, ':': True, 'fw': True, 'nice': True, 'mhoter': True, 'fucking': True, \n",
    "\t\t'top': True, 'of': True, 'the': True, 'morning': True, 'to': True, 'you': True, '!': True,\n",
    "\t\t')': True, 'matayaa': True}, 'spam')\n",
    "\t\t\"\"\"\n",
    "\t\tdata_directory = \"data\"\t\t\n",
    "\t\thamList = []\n",
    "\t\tspamList = []\n",
    "\t\tfor directories, subdirs, files in os.walk(data_directory):\n",
    "\t\t\tif (os.path.split(directories)[1]  == 'ham'):\n",
    "\t\t\t\tfor fileName in files:      \n",
    "\t\t\t\t\twith open(os.path.join(directories, fileName), encoding=\"latin-1\") as f:\n",
    "\t\t\t\t\t\tmessage = f.read()\t\t\t\t\t\n",
    "\t\t\t\t\t\twords = word_tokenize(message)\t\t\t\t\t\t\n",
    "\t\t\t\t\t\thamList.append((self.createWordFeatures(words), \"ham\"))\n",
    "\t\t\t\n",
    "\t\t\tif (os.path.split(directories)[1]  == 'spam'):\n",
    "\t\t\t\tfor fileName in files:\n",
    "\t\t\t\t\twith open(os.path.join(directories, fileName), encoding=\"latin-1\") as f:\n",
    "\t\t\t\t\t\tmessage = f.read()\t\t\t\t\t\t\n",
    "\t\t\t\t\t\twords = word_tokenize(message)\t\t\t\t\t\t\n",
    "\t\t\t\t\t\tspamList.append((self.createWordFeatures(words), \"spam\"))\n",
    "\t\tmergedList = hamList + spamList\n",
    "\t\trandom.shuffle(mergedList)\t\t\n",
    "\t\treturn mergedList\n",
    "\n",
    "\tdef createTestTrain(self):\n",
    "\t\t\"\"\"\n",
    "\t\tFunction\n",
    "\t\t--------\n",
    "\t\tcreateTestTrain\n",
    "\n",
    "\t\tCreate test/train splits.\n",
    "\n",
    "\t\tReturns\n",
    "\t\t-------\n",
    "\t\t(trainingData, testData) : tuple\n",
    "\t\t\"\"\"\n",
    "\t\tmergedList = self.tokenizeCreateWordFeatures()\n",
    "\t\ttrainingPart = int(len(mergedList) * .6)\n",
    "\t\ttrainingData = mergedList[:trainingPart]\n",
    "\t\ttestData =  mergedList[trainingPart:]\t\t\n",
    "\t\treturn (trainingData, testData)\n",
    "\n",
    "\n",
    "\tdef createModel(self):\n",
    "\t\t\"\"\"\n",
    "\t\tFunction\n",
    "\t\t--------\n",
    "\t\tcreateModel\n",
    "\n",
    "\t\tCreate the naive Bayes classifier\n",
    "\n",
    "\t\tReturns\n",
    "\t\t-------\n",
    "\t\tclassifier : nltk.classify.naivebayes.NaiveBayesClassifier\n",
    "\t\t\"\"\"\n",
    "\t\ttrainingData, testData = self.createTestTrain()\t\t\n",
    "\t\tclassifier = NaiveBayesClassifier.train(trainingData)\n",
    "\t\treturn classifier\n",
    "\n",
    "\tdef saveModel(self):\n",
    "\t\t\"\"\"\n",
    "\t\tFunction\n",
    "\t\t--------\n",
    "\t\tsaveModel\n",
    "\n",
    "\t\tSave the model to disk\n",
    "\n",
    "\t\tReturns\n",
    "\t\t-------\n",
    "\t\toutputMsg : str\n",
    "\t\t\n",
    "\t\tExample\n",
    "\t\t-------\n",
    "\t\tsaveModel()\n",
    "\t\t'Model saved successfully !'\n",
    "\t\t\"\"\"\n",
    "\t\tclassifier = self.createModel()\n",
    "\t\tfileName = 'nb_model.sav'\n",
    "\t\toutputMsg = \"\"\n",
    "\t\ttry:\n",
    "\t\t\tpickle.dump(classifier, open(fileName, 'wb'))\n",
    "\t\t\toutputMsg = \"Model saved successfully !\"\n",
    "\t\texcept:\n",
    "\t\t\toutputMsg = \"Error when saving model !\"\n",
    "\t\treturn outputMsg\n",
    "\n",
    "\tdef predictMessage(self,msg):\n",
    "\t\t\"\"\"\n",
    "\t\tFunction\n",
    "\t\t--------\n",
    "\t\tpredictMessage\n",
    "\n",
    "\t\tClassify the message by telling if it is a ham or spam.\n",
    "\n",
    "\t\tParameters\n",
    "\t\t----------\n",
    "\t\tmsg : str\n",
    "\t\tA message we want to classify\n",
    "\t\t\n",
    "\t\tReturns\n",
    "\t\t-------\n",
    "\t\tstr: 'ham' or 'spam'\n",
    "\t\t\n",
    "\t\tExample\n",
    "\t\t-------\n",
    "\t\tmessage = \"Hi welcome to this session, please log in using your username and password then click on the start button.\"\n",
    "\t\tpredictMessage(message)\n",
    "\t\t\n",
    "\t\t'ham'\n",
    "\t\t\"\"\"\n",
    "\t\tfileName = 'nb_model.sav'\n",
    "\t\tloaded_model = pickle.load(open(fileName, 'rb'))\n",
    "\t\toutput_msg = \"\"\n",
    "\t\twords = word_tokenize(msg)\n",
    "\t\toutput_msg = dict( [ (word, True) for word in words] )\n",
    "\t\treturn loaded_model.classify(output_msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## App ~ Flask Server\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, request, render_template, json, redirect, url_for, session, abort, Markup\n",
    "from engine import SpamDetector\n",
    "app = Flask(__name__)\n",
    "app.secret_key = 'F12Zr47j\\3yX R~X@H!jmM]Lwf/,?KT'\n",
    "\n",
    "# Defining the basic route and its corresponding request handler\n",
    "@app.route(\"/\")\n",
    "def index():\n",
    "\n",
    "    return render_template('index.html')\n",
    "\n",
    "\n",
    "# Run the engine\n",
    "@app.route(\"/RunEngine\", methods=['POST', 'GET'])\n",
    "def RunEngine():\n",
    "\n",
    "    clearsession()\n",
    "    OutputMsg = None\n",
    "    message = None\n",
    "\n",
    "    if request.form['inputMessage'] !=\"\":\n",
    "\n",
    "        message = request.form['inputMessage']\n",
    "        spam_detector = SpamDetector()\n",
    "        OutputMsg = spam_detector.predictMessage(message)\n",
    "        if OutputMsg == \"ham\":\n",
    "            OutputMsg = Markup(\"<img src='../static/img/ham.png' width='50' height='50'/><br>No worries, your message is fine ðŸ˜‰\")\n",
    "        else:\n",
    "            OutputMsg = Markup(\"<img src='../static/img/spam.png' width='50' height='50'/><br>Be careful! your message is a spam!\")\n",
    "        return render_template('index.html', message=message, OutputMsg=OutputMsg)\n",
    "\n",
    "    else:\n",
    "        return redirect(url_for('index'))\n",
    "\n",
    "# Clear the session \n",
    "@app.route('/clear')\n",
    "def clearsession():\n",
    "    # Clear the session\n",
    "    session.clear()\n",
    "    # Redirect the user to the main page\n",
    "    return redirect(url_for('index'))\n",
    "\n",
    "# Build the model \n",
    "@app.route('/build')\n",
    "def build_model():\n",
    "    spam_detector = SpamDetector()\n",
    "    build_results = spam_detector.saveModel()\n",
    "    return build_results\n",
    "\n",
    "# Predict the message via command line\n",
    "@app.route('/predict/<message>')\n",
    "def predict_message(message):\n",
    "    spam_detector = SpamDetector()\n",
    "    return spam_detector.predictMessage(message)\n",
    "\n",
    "# Checking if the executed file is the main program and run the app\t\n",
    "#if __name__ == \"__main__\":\n",
    "    #app.debug=True\n",
    "    #app.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Page\n",
    "***\n",
    "<img src=\"jupyter_images/webpage.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's Run It\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python app.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spamector as API\n",
    "***\n",
    "\n",
    "We have the possibility to use Spamector as an API. Here is an example using [Postman](https://www.getpostman.com/) and we are trying to classify the message passed in the URL.\n",
    "<img src=\"jupyter_images/postman.png\"/>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
